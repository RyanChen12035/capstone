{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyME0GyZCJruN09qKiFNoN/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanChen12035/capstone/blob/main/Llama2_synthesizing_wNER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download Llama 2-13b-4bits-quantized-chat version, and try out different ways of prompting to ask the model generate synthetic data based on BIOS tagging."
      ],
      "metadata": {
        "id": "3nC98XpdZoXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4"
      ],
      "metadata": {
        "id": "CIJy1nepZ5vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\""
      ],
      "metadata": {
        "id": "UgcdB5AbZ-If"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "SaXobiJsaA43"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the model\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "#move to GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Maximum characters\n",
        "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
        "    )"
      ],
      "metadata": {
        "id": "GaHuZroTaEzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Llama2 prompt template\n",
        "\n",
        "according to https://www.youtube.com/watch?v=Pb_RGAl75VE\n",
        "\n",
        "The impact of chat templates on the performance of the model is unclear. In most cases, we fine-tune base models that have not been trained with a particular template, which is also why there's no clear standard. However, they are important as they can cause many issues and limit the compatibility of your models.\n",
        "\n",
        "    <s>[INST] <<SYS>>\n",
        "    You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "    <</SYS>>\n",
        "\n",
        "    There's a llama in my garden ðŸ˜± What should I do? [/INST]"
      ],
      "metadata": {
        "id": "mS_ZnCDsfynM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It's a chat version so we need to follow the template of Llama 2\n",
        "# Run text generation pipeline with our model\n",
        "system_prompt = \"You are an intelligent Reversed Named Entity Recognition (NER) system. Please synthesize data based on the definition of NER tagging provided and replace the tagging. The output should be in the given format as examples but don't use the tagging in the example. Please be creative but still keep the output reasonable.\"\n",
        "sentence_NER = \" {ORG} rejects {PERSON} call to boycott {LOC} lamb\"\n",
        "prompt = \"\"\"Definition of NER Tagging:\\n\n",
        "            1. {PERSON}: Short name or full name of a person from any geographic regions.\\n\n",
        "            2. {DATE}: Any format of dates. Dates can also be in natural language.\\n\n",
        "            3. {LOC}: Name of any geographic location, like cities, countries, continents, districts etc.\\n\n",
        "            4. {ORG}: Name of the companies like Google, samsung, Apple etc.\\n\n",
        "            5. {NUMBERS}: Numerical entites which are numerically present or mentioned in words like 7000, half of dozen etc.\\n\n",
        "            Examples:\\n\n",
        "            1. Sentence: {LOC} and {LOC} are friends. G20 summit going to held in {LOC} in {DATE}. Indian Prime Minister {PERSON} will be hosting it and {ORG} will be giving charity of {NUMBERS}.\\n\n",
        "            Output: Sentence: USA and India are friends. G20 summit going to held in India in September 2023. Indian Prime Minister Narendra Modi will be hosting it and TATA will be giving charity of $150 Million.\\n\n",
        "\n",
        "        \"\"\"\n",
        "prompt_template = f\"<s>[INST] <<SYS>>:\\n{system_prompt}\\n<</SYS>>\\n{prompt}\\n Please synthesize the data and replace the NER tagging:{sentence_NER}[/INST]\\n\""
      ],
      "metadata": {
        "id": "bMYZHwChaNNj"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NER training material from https://huggingface.co/datasets/conll2003?row=0\n",
        "# example:EU rejects German cal to boycott British lamb\n",
        "# BIOS tagging: {ORG} rejects {LOC} cal to boycott {LOC} lamb"
      ],
      "metadata": {
        "id": "iTBaspfglOXq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=lcpp_llm(prompt=prompt_template, max_tokens=512, temperature=0.5, top_p=0.95,\n",
        "                  repeat_penalty=1.2, top_k=150,\n",
        "                  echo=True)\n",
        "# higher temperature, more creative response."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdWp3nOybsHu",
        "outputId": "c64b13a5-cd2b-4314-8620-d3e4902ec670"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"][len(prompt_template):])\n",
        "\n",
        "#temp = 1   The tech giant Apple has rejected a call by its CEO Tim Cook to boycott the upcoming G20 summit in India. According to sources close to the company, Apple is concerned that such a move could harm their business and relationships with Indian leaders. The company is instead choosing to focus on donating $150 million to support education initiatives in underserved communities across India.\n",
        "#temp = 0.9 Apple rejects Tim Cook's call to boycott California lamb.\n",
        "#temp = 0.5 pple rejects Tim Cook's call to boycott Cupertino lamb.\n",
        "#temp = 0.3 Apple rejects Tim Cook's call to boycott California lamb.\n",
        "#temp = 0.1 Apple rejects Tim Cook's call to boycott California lamb."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D92o4dxdDG3",
        "outputId": "2a151b8d-ec32-4f10-c3ba-92efe8f6a101"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure, here's the synthesized output based on the given definition of NER tagging:\n",
            "\n",
            "Apple rejects Tim Cook's call to boycott California lamb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Given a sentence with private data, annotate and replace it with NER tagging"
      ],
      "metadata": {
        "id": "jcYO2BCLuxUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It's a chat version so we need to follow the template of Llama 2\n",
        "# Run text generation pipeline with our model\n",
        "system_prompt = \"You are an intelligent Named Entity Recognition (NER) system. Please annotate and replace the input sentence accordingly with the definition of the NER tagging provided. The output should be in given format with examples.\"\n",
        "sentence = \"EU rejects German cal to boycott British lamb\"\n",
        "prompt = \"\"\"Definition of NER tagging:\\n\n",
        "            1. {PERSON}: Short name or full name of a person from any geographic regions.\\n\n",
        "            2. {DATE}: Any format of dates. Dates can also be in natural language.\\n\n",
        "            3. {LOC}: Name of any geographic location, like cities, countries, continents, districts etc.\\n\n",
        "            4. {ORG}: Name of the companies like Google, samsung, Apple etc.\\n\n",
        "            5. {NUMBERS}: Numerical entites which are numerically present or mentioned in words like 7000, half of dozen etc.\\n\n",
        "            Examples:\\n\n",
        "            1. Sentence: Sentence: USA and India are friends. G20 summit going to held in India in September 2023. Indian Prime Minister Narendra Modi will be hosting it and TATA will be giving charity of $150 Million.\\n\n",
        "            Output: {LOC} and {LOC} are friends. G20 summit going to held in {LOC} in {DATE}. Indian Prime Minister {PERSON} will be hosting it and {ORG} will be giving charity of {NUMBERS}.\\n\n",
        "\n",
        "        \"\"\"\n",
        "prompt_template = f\"<s>[INST] <<SYS>>:\\n{system_prompt}\\n<</SYS>>\\n{prompt}\\n Please annotate the input with NER tagging and replace it based on the definition of the NER tagging:{sentence}[/INST]\\n\""
      ],
      "metadata": {
        "id": "wfuD_OArvD3t"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=lcpp_llm(prompt=prompt_template, max_tokens=512, temperature=0.5, top_p=0.95,\n",
        "                  repeat_penalty=1.2, top_k=150,\n",
        "                  echo=True)\n",
        "# higher temperature, more creative response."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8us2XoI1v1qc",
        "outputId": "bc6e7c83-00ba-41df-b752-2fea37b3ea5b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"][len(prompt_template):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3L0oPVmv23w",
        "outputId": "515004e5-41c1-43f2-fadb-2786b752c41d"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sure! Here's the input sentence with NER tags added, based on the definition you provided:\n",
            "\n",
            "Input sentence: EU rejects German call to boycott British lamb.\n",
            "\n",
            "NER-tagged output:\n",
            "\n",
            "{ORG} rejects {PERSON}'s call to boycott {LOC} lamb.\n",
            "\n",
            "Here's a breakdown of each tag:\n",
            "\n",
            "* {ORG}: European Union (EU)\n",
            "* {PERSON}: German (German government or representative)\n",
            "* {LOC}: Britain (United Kingdom)\n",
            "* {NUMBERS}: 150 million (the amount of charity\n"
          ]
        }
      ]
    }
  ]
}